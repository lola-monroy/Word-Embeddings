{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pràctica 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenament de models de Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "     ---------------------------------------- 0.0/542.0 kB ? eta -:--:--\n",
      "     ------------- ------------------------ 194.6/542.0 kB 3.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- -- 501.8/542.0 kB 6.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 542.0/542.0 kB 5.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.12.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.25.2)\n",
      "Collecting pyarrow>=12.0.0\n",
      "  Downloading pyarrow-16.1.0-cp311-cp311-win_amd64.whl (25.9 MB)\n",
      "     ---------------------------------------- 0.0/25.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.2/25.9 MB 5.0 MB/s eta 0:00:06\n",
      "      --------------------------------------- 0.6/25.9 MB 6.2 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 0.9/25.9 MB 6.1 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 1.2/25.9 MB 6.2 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 1.4/25.9 MB 6.0 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 1.7/25.9 MB 6.0 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 2.0/25.9 MB 6.0 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 2.2/25.9 MB 6.1 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 2.5/25.9 MB 6.0 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 2.7/25.9 MB 5.9 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 2.9/25.9 MB 5.8 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 3.2/25.9 MB 5.7 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 3.3/25.9 MB 5.6 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 3.6/25.9 MB 5.6 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 3.8/25.9 MB 5.6 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 4.0/25.9 MB 5.4 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 4.3/25.9 MB 5.5 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 4.6/25.9 MB 5.5 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 4.9/25.9 MB 5.6 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 5.2/25.9 MB 5.6 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 5.4/25.9 MB 5.6 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 5.7/25.9 MB 5.7 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 6.0/25.9 MB 5.6 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 6.2/25.9 MB 5.6 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 6.5/25.9 MB 5.6 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 6.6/25.9 MB 5.5 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 6.8/25.9 MB 5.5 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 7.0/25.9 MB 5.5 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 7.2/25.9 MB 5.4 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 7.3/25.9 MB 5.3 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 7.6/25.9 MB 5.4 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 7.9/25.9 MB 5.3 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 8.2/25.9 MB 5.4 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 8.4/25.9 MB 5.4 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 8.7/25.9 MB 5.4 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 9.0/25.9 MB 5.4 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 9.3/25.9 MB 5.4 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 9.6/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 9.9/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 10.1/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 10.4/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 10.7/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 11.0/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 11.2/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 11.5/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 11.7/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 12.0/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 12.4/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 12.7/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 13.0/25.9 MB 5.6 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 13.3/25.9 MB 5.6 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 13.6/25.9 MB 5.7 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 13.9/25.9 MB 5.7 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 14.2/25.9 MB 5.8 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 14.6/25.9 MB 5.8 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 14.9/25.9 MB 5.8 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 15.1/25.9 MB 5.8 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 15.5/25.9 MB 5.9 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 15.8/25.9 MB 5.9 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 16.1/25.9 MB 6.0 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 16.3/25.9 MB 5.9 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 16.7/25.9 MB 6.0 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 17.0/25.9 MB 6.1 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 17.3/25.9 MB 6.3 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 17.6/25.9 MB 6.3 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 17.8/25.9 MB 6.3 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 18.1/25.9 MB 6.4 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 18.5/25.9 MB 6.4 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 18.8/25.9 MB 6.4 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 19.1/25.9 MB 6.4 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 19.4/25.9 MB 6.4 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 19.7/25.9 MB 6.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 19.9/25.9 MB 6.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 20.1/25.9 MB 6.3 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 20.3/25.9 MB 6.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 20.6/25.9 MB 6.3 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 20.8/25.9 MB 6.2 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 21.0/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 21.1/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 21.4/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 21.6/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 21.9/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 22.1/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 22.5/25.9 MB 6.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 22.8/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 23.0/25.9 MB 6.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 23.3/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 23.6/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 23.9/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 24.3/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 24.5/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 24.8/25.9 MB 6.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 25.0/25.9 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  25.4/25.9 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  25.7/25.9 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  25.9/25.9 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 25.9/25.9 MB 5.7 MB/s eta 0:00:00\n",
      "Collecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "     ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 116.3/116.3 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.66.2)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl (29 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "     ---------------------------------------- 0.0/143.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 143.5/143.5 kB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2024.3.1)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.9.5-cp311-cp311-win_amd64.whl (370 kB)\n",
      "     ---------------------------------------- 0.0/370.8 kB ? eta -:--:--\n",
      "     ----------------------- -------------- 225.3/370.8 kB 6.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 370.8/370.8 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "     ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.5/50.5 kB ? eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.5-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl (76 kB)\n",
      "     ---------------------------------------- 0.0/76.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 76.7/76.7 kB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, multidict, frozenlist, dill, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 datasets-2.19.1 dill-0.3.8 frozenlist-1.4.1 multidict-6.0.5 multiprocess-0.70.16 pyarrow-16.1.0 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for projecte-aina/catalan_general_crawling contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/projecte-aina/catalan_general_crawling\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c90be95886412686151f7a6bfcd35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885e8832edfc49caab4822b72ad2a32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a40a89e8e947db88aa3466dec5dc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/875M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d26cdbbcde42e787edd7cea04a62be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"projecte-aina/catalan_general_crawling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1016113\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1016113\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Reduïu els costos dels processos administratius al vostre organisme públic\\nEviteu els desplaçaments i pèrdua de temps als ciutadans en les seves gestions\\nOferiu una administració més transparent a ciutadans i empreses\\nEns grans i petits experimenten aquesta transformació amb èxit, gràcies al suport de l\\'AOC\\nDepartament de Sistemes d\\'Informació i Processos\\n\" Via Oberta ens ha permès fer efectiu el dret dels ciutadans a no aportar documents, eliminant paper i simplificant procediments\"\\n\" e.FACT proporciona informació indispensable per a la realització de les auditories del registre comptable de factures de les Administracions Públiques Catalanes\"\\nCoordinador del departament d\\'Informàtica\\n\"El servei VIA OBERTA és el que ha aportat majors avantatges per als ciutadans\"\\n\"Amb l\\' e-NOTUM hem escurçat els procediments en 12 dies, quasi un 40% menys!\"\\nCoordinadora d\\'organització de persones i e-administració\\n\" Via Oberta ofereix millores per als ciutadans al no haver d\\'aportar cap document\"\\nResponsable d\\'Informàtica i Administració Electrònica\\n\" e-TRAM ens ha permès implantar un servei de tramitació electrònica per als ciutadans de forma ràpida, senzilla i amb un cost reduït\"\\n\"Els municipis amb pocs habitants trobem en els serveis de l\\'AOC la gratuïtat i la comoditat necessàries per dur a terme el nostre dia a dia\"\\n\"Les T-CAT han permès incorporar de forma segura la signatura electrònica dins dels nostres procediments afavorint la transformació digital de la nostra activitat\"\\nCap de Departament de Sistemes i Tecnologies de la Informació\\n\"Amb el desplegament de l\\' idCAT hem apropat l\\'Ajuntament a la ciutadania\"\\n\"Mitjançant els serveis de Govern Obert de l\\'AOC hem pogut fer fàcil el que sembla difícil\"\\n\"Al tauler electrònic pots penjar fins i tot el projecte sencer i al final et permet fer també la diligència\"\\nÀrea de Promoció Econòmica, Administració i Hisenda\\n\"El Sobre Digital i la PSCP han aconseguit una comunió senzilla entre empreses i administració per universalitzar la compra pública electrònica\"\\n\"L\\' e-SET és la implantació d\\'un nou sistema de treball que facilita la feina del dia a dia\"\\nCap del servei de contractació i compres\\n\"El Sobre Digital, una experiència imprescindible per a la bona administració amb estalvi de recursos i millora de la seguretat jurídica i la transparència\"\\nÀrea d\\'Organització i Administració Electrònica\\n\"El desplegament de la valisa electrònica ha estat clau en el procés de transformació digital dels nostres procediments interns\"\\n\"L\\' Hèstia permet el treball en temps real i des de qualsevol lloc, així com sistematitzar la pràctica professional, recollir la informació ordenadament i amb el mateix llenguatge\"\\nConsulta els materials del Congrés de Govern Digital 2019\\nGoverns transparents, fluids, dinàmics, líquids... un bon lema pel principal objectiu de la governança del segle XXI: democratitzar-ho tot.\\nConfluències, rius, cooperació.\\nCatalunya, Mediterrània, mar de drets.\\nA favor: totes les Administracions movent-se per posar-se al dia i millorar, tot aprofitant la revolució digital.\\nEn contra: quants cops estem reinventant la roda i quantes quantes oportunitats perdudes de fer-ho una única vegada i de forma coordinada i col·laborativa?\\n\"La transparència és una oportunitat.\\nHem de perdre tota por a explicar què fem\": la conclusió de la taula d\\'alcaldies de la Jornada de Govern Obert pic.twitter.com/ERbgLSIXZM\\nEl director general de Participació Ciutadana ens convida a transformar les administracions públiques a partir de la participació ciutadana\\nEns cal que allò que preocupa i ocupa els governants formi part d\\'allò en què participa la ciutadania pic.twitter.com/NwQr4EZSCS: \"A moltes institucions encara els sona xinés això de les dades obertes i la transparència.\\nDe que serveix que hi hagi un portal, si llavors no hi ha dades?\\nLlavors l\\'accés a la informació pels periodistes és molt parcial\".\\nOferim eines que, conjuntament amb la metodologia i el suport necessari, fan possible l\\'assoliment d\\'un govern digital\\nPosem al vostre abast tot el coneixement: formació, guies, normatives, etc.\\nTenim eines per gestionar àgilment part del procés administratiu del vostre ens\\nEl nostre equip farà tot el possible per resoldre les vostres incidències\\nSabem que es tracta d\\'una decisió molt important per al vostre ens i és per això que us ho volem posar fàcil.\\nLa selecció de l\\'actualitat d\\'Administració Oberta a la vostra safata.',\n",
       "  'En compliment de la Directiva 2009/136/CE, desenvolupada en el nostre ordenament per l\\'article 22.2 de la Llei de Serveis de Societat de la Informació (LSSI) i seguint les instruccions de l\\'Agència Espanyola de Protecció de Dades, procedim a informar-li detalladament de l\\'ús que es realitza a la nostra pàgina web.\\nAquesta informació no revela la seva identitat, però sí que permet la seva identificació com a un usuari concret i pot guardar informació relativa a la freqüència amb la que visita la pàgina web, les seves preferències de navegació o aquella informació que més l\\'interessa.\\nEl que ens permet, cada vegada que accedeix a www.aoc.cat, millorar la qualitat i la usabilitat de la nostra pàgina web.\\nNo obstant, si les desactiva, pot ser que la seva navegació per www.aoc.cat no sigui òptima i algunes de les seves utilitats no funcionin correctament.\\nCookies analítiques: galetes de Google Analytics\\nAquesta pàgina web utilitza Google Analytics, un servei analític del web prestat per Google.\\nInc, una companyia de Delaware l\\'oficina principal de la qual es troba a 1600 Amphitheatre Parkway, Mountain View (Califòrnia), CA 94043, Estats Units (\" Google\").\\nLa informació que genera la cookie sobre l\\'ús del lloc web (incloent l\\'adreça IP) serà directament transmesa i arxivada per Google en els seus servidors d\\'Estats Units.\\nGoogle utilitzarà aquesta informació per compte nostre amb el propòsit de seguir la pista del seu ús del lloc web.\\nGoogle podrà transmetre aquesta informació a tercers quan així ho requereixi la legislació, o quan aquests tercers processin la informació per compte de Google.\\nEn aquests casos, Google no associarà la seva adreça IP amb cap altra dada de què disposi.\\nEn utilitzar aquesta pàgina web consent el tractament de la seva informació per Google en la forma i per als fins anteriorment indicats.\\nL\\'exercici de qualsevol dret s\\'haurà de realitzar mitjançant comunicació directa amb Google.\\nPer optar per no ser rastrejats per Google Analytics a través de tots els llocs web podeu consultar http://tools.google.com/dlpage/gaoptout\\nAixí mateix, també registra quan va ser la primera i l\\'última vegada que l\\'usuari va visitar el web www.aoc.cat.\\nGaletes en altres llocs web del Consorci AOC\\nLes seves finalitats són descrites a la pàgina de privacitat de Twitter.\\nConfiguració de l\\'usuari per evitar Cookies\\nÉs cas de dubte pot dirigir-se al webmaster del domini creador de la cookie.\\nLa selecció de l\\'actualitat d\\'Administració Oberta a la vostra safata.',\n",
       "  'L\\'ús de la informació continguda en aquest lloc web implica l\\'acceptació i el consentiment en els termes i les condicions que es detallen en aquest avís legal.\\nTitularitat i règim de responsabilitat de la pàgina web\\nEl responsable d\\'aquesta pàgina web és el Consorci Administració Oberta de Catalunya, (Consorci AOC), amb NIF Q0801175A, i ubicat al Carrer de Tànger, núm. 98, (planta baixa) 08018 (tel. 93 272 25 00 i fax.\\nTota persona que accedeixi a aquest lloc web assumeix el paper d\\'usuari, comprometent-se a l\\'observança i compliment rigorós de les disposicions aquí disposades, així com a qualsevol altre disposició legal que li sigui d\\'aplicació.\\nEl Consorci AOC té el dret a modificar la informació que apareix en aquesta pàgina web, sense que existeixi la obligació de preavís o posada en coneixement dels usuaris de les noves obligacions -a excepció dels compromisos assumits en virtut de convenis específics – entenent-se com a suficient la seva publicació en el lloc web.\\nLes informacions i els continguts relacionats amb l\\'actuació i les funcions del Consorci AOC, que s\\'inclouen en aquest web, estan subjectes a les previsions següents:\\nResponsabilitat amb relació als continguts\\nEl Consorci AOC treballa perquè les informacions, els continguts i els serveis oferts o difosos en aquest web acompleixin de manera suficient la necessària integritat, veracitat, actualització, accessibilitat i usabilitat.\\nA aquest efecte, cal tenir en compte la data d\\'actualització de cadascun dels continguts que en cada cas s\\'indiqui.\\nLa pàgina web ofereix informació, consells, guies i d\\'altres continguts preparats pel Consorci AOC amb finalitats de difusió, informació, conscienciació i en determinats casos la prestació de serveis específics d\\'administració electrònica.\\nS\\'informa a l\\'usuari que tots aquests continguts, malgrat estar preparats amb el màxim nivell de qualitat possibles, no poden suposar en cap moment assessorament específic en matèria tecnològica i/o jurídica o ser considerats com a actuacions dirigides a la resolució de problemàtiques específiques.\\nEn qualsevol cas, el Consorci AOC es reserva el dret a modificar-los, suprimir-los, desenvolupar-los o actualitzar-los unilateralment sense notificació prèvia i sense assumir cap responsabilitat.\\nEl Consorci AOC ofereix la traducció automàtica en altres llengües diferents del català dels continguts del seu web per tal de facilitar als ciutadans la comprensió del text en el seu propi idioma.\\nMalgrat tot, els articles traduïts automàticament poden contenir errors materials dels quals el Consorci AOC no se\\'n fa responsable.\\nEn l\\'actualitat, el Consorci AOC només garanteix la veracitat dels continguts en llengua catalana.\\nReferències i enllaços a webs d\\'altres organitzacions\\nEl web del Consorci AOC conté referències o enllaços a webs de tercers (\"links\"), la major part d\\'elles són a pàgines d\\'Internet d\\'altres administracions públiques, que s\\'han considerat d\\'interès pels usuaris.\\nEn el cas que s\\'abandoni el web, el Consorci AOC no assumeix cap responsabilitat derivada de la connexió o dels contingut dels enllaços de tercers.\\nTot i això, es revisen periòdicament els enllaços a altres pàgines per tal d\\'evitar la inclusió d\\'enllaços que no compleixen la normativa de protecció de dades, així com la resta de normativa vigent.\\nEn aquest sentit, el Consorci manifesta que si es detecta qualsevol contingut que pugui contravenir la legislació nacional o internacional, o l\\'ordre públic, es procedirà a la retirada immediata de l\\'enllaç, posant-ho en coneixement de les autoritat competents.\\nEl Consorci AOC no es fa responsable de la informació i continguts emmagatzemats, a títol enunciatiu però no limitatiu, en els fòrums, blocs, comentaris en xarxes socials, o qualsevol altre mitjà del Consorci AOC que permeti a tercers publicar continguts de forma independent.\\nNo obstant, en compliment de l\\'article 11 i 16 de la LSSICE, es posa a disposició de tots els usuaris, autoritats i forces de seguretat, per col·laborar de forma activa en la retirada o, en el seu cas, bloqueig de tots aquells continguts que poguessin afectar o contravenir la legislació nacional, o internacional, drets de tercers o la moral i l\\'ordre públic.\\nEn el cas que qualsevol usuari consideri que existeix en el lloc web algun contingut que pugui ser susceptible de l\\'anterior classificació, es sol·licita que ho comuniqui de forma immediata a l\\'administrador del la pàgina web per mitjà del correu electrònic [EMAIL]\\nReproducció de continguts propis\\nEl Consorci AOC facilita la consulta lliure i gratuïta de la informació continguda en el web i autoritza la reproducció total o parcial dels seus continguts, sempre i quan els continguts esmentats es conservin íntegres, es citi la font i la data en la que s\\'ha realitzat la còpia, no es manipulin, ni alterin els continguts i no s\\'utilitzi directament amb finalitats comercials (Llei 37/2007, de 16 de novembre, sobre la reutilització de la informació del sector públic).\\nEl Consorci AOC autoritza la descarrega gratuïta dels manuals, impresos, programes i publicacions informatives que s\\'inclouen en el seu web, a efectes de la seva reproducció i distribució, llevat que s\\'indiqui el contrari de forma expressa.\\nNo obstant això, en determinats supòsits el Consorci AOC pot indicar de manera explícita que és necessari sol·licitar una autorització expressa.\\nAixí mateix, la reutilització es pot limitar per la tutela d\\'altres béns jurídics prioritaris, com ara la protecció de les dades personals, la intimitat o els drets de protecció intel·lectual de tercers.\\nEl domini d\\'aquest web és titularitat del Consorci AOC, així com els drets de propietat intel·lectual, el seu disseny i els codis que conté, llevat que s\\'indiqui una titularitat diferent.\\nNo s\\'autoritza en cap cas, l\\'ús de marques o signes distintius, logotips i en general símbols distintius de qualsevol naturalesa propietat del Consorci AOC, en publicacions i webs que no siguin d\\'ens participats o patrocinats per aquest Consorci, sense el coneixement i l\\'autorització corresponent del Consorci AOC.\\nEl Consorci AOC no assumirà cap responsabilitat derivada de l\\'ús per part de tercers del contingut d\\'aquesta pàgina Web i podrà exercitar totes les accions civils o penals que li corresponguin en cas d\\'infracció d\\'aquests drets per part de l\\'usuari.\\nAquesta informació es troba continguda a la Política de Cookies del Consorci AOC.\\nDret aplicable i jurisdicció competent\\nLa llei aplicable en cas de disputa o conflicte d\\'interpretació dels termes que conforme aquest Avís legal, així com qualsevol aspecte relacionat amb els serveis d\\'aquest web, serà la legislació espanyola.\\nEls possibles conflictes relatius a aquest web es regiran exclusivament pel dret espanyol, essent els jutjats de Barcelona els únics competents.\\nTota persona usuària del web, independentment de la jurisdicció territorial des de la qual es produeixi el seu accés, accepta el compliment i respecte d\\'aquesta clàusula amb renúncia expressa a qualsevol altre fur que li pogués correspondre.\\nSi alguna part o clàusula d\\'aquestes Condicions fos declarada nul·la o deixada sense efecte per una resolució judicial, les restants estipulacions conservaran la seva validesa.\\nLa selecció de l\\'actualitat d\\'Administració Oberta a la vostra safata.',\n",
       "  \"Els Reconeixements Administració Oberta als ajuntaments i consells comarcals que atorga anualment l'AOC han esdevingut el reconeixement públic de tots aquells ens que destaquen en la transformació digital de la seva relació amb la ciutadania i en la seva gestió interna.\\nEls guardons s'atorguen en base a uns indicadors objectius, l'anàlisi dels webs dels ens i l'ús de determinats serveis del Consorci AOC.\\nEl seu objectiu és valorar i reconèixer la implantació i l'ús dels serveis d'administració electrònica i, tal i com s'ha indicat anteriorment, la seva conseqüent transformació digital en seva relació amb la ciutadania i en la seva gestió interna.\\nEls Reconeixements Administració Oberta tenen set categories d'acord amb el nombre d'habitants dels ens i la seva naturalesa:\\nDescripció del mètode d'avaluació dels Reconeixements Administració Oberta\\nLlistat de guardonats a l'edició del 2018: ajuntaments i consells comarcals capdavanters en administració digital\\nSegells Reconeixements Administració Oberta 2018\\nLa selecció de l'actualitat d'Administració Oberta a la vostra safata.\",\n",
       "  \"En el marc del desenvolupament de l'Administració electrònica, des de l'AOC copsem l'estat de l'administració pel que fa als instruments municipals d'Administració electrònica i, més específicament, al reconeixement efectiu dels drets dels ciutadans arrel de la normativa que regula l'ús dels mitjans electrònics, la transparència, l'accés a la informació pública i el bon govern.\\nAmb aquest objectiu, periòdicament duem a terme una revisió de l'estat de l'e-Administració als 947 ajuntaments de Catalunya 1 i publiquem les dades en un informe.\\nAixí mateix, els resultats de l'anàlisi es plasmen en un mapa de Catalunya interactiu.\\nAl mapa municipal català podeu accedir a la informació relacionada amb els 947 municipis catalans:\\nLa informació que mostren els mapes és la més recent.\\nSi voleu conèixer les dades dels estudis anteriors podeu accedir als informes sobre administració.\\nTambé podeu consultar les dades dels 42 consells comarcals al mapa d'e-administració per comarques.\\nEls informes sobre e-Administració són el resultat d'un estudi dut a terme pel personal del Gabinet Tècnic de l'AOC en un període de temps concret i en base a:\\nA continuació detallem els paràmetres analitzats a l'estudi, agrupats en sis blocs\\nSi l'ajuntament ha aprovat normativa en matèria d'administració electrònica\\nSi l'ajuntament ha aprovat alguna norma específica per regular l'ús dels mitjans electrònics en la seva administració (ordenança reguladora d'administració electrònica, registre electrònic i seu electrònica).\\nEl resultat d'aquesta anàlisi es contrasta amb el personal tècnic dels consells comarcals, d'acord amb el conveni de col·laboració que hi ha entre els consells i el Consorci AOC.\\n1 En els tres primers informes de 2010, es van recollir dades de 946 ajuntaments, ja que La Canonja encara no era legalment municipi independent.\\nTambé es va analitzar en el seu moment el municipi de Medinyà fins a la seva forçosa desaparició el febrer de 2018.\\nEvolució de les dades dels informes sobre l'e-Administració (desembre 2017) (977 kB)\\nNOTA: les dades recollides en aquests informes es publiquen ara en format de dades obertes a indicadors públics d'activitat\\nLa selecció de l'actualitat d'Administració Oberta a la vostra safata.\"]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1016113\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def dividir_dataset(dataset, output_dir, tamano_partes):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    total_bytes = 0\n",
    "    contador = 1\n",
    "    current_size = 0\n",
    "    current_part = []\n",
    "    \n",
    "    for i, row in enumerate(dataset):\n",
    "        text = row['text']\n",
    "        current_size += len(text.encode('utf-8'))\n",
    "        current_part.append(text)\n",
    "        \n",
    "        if current_size >= tamano_partes[contador - 1]:\n",
    "            with open(os.path.join(output_dir, f'parte_{contador}.txt'), 'w', encoding='utf-8') as f:\n",
    "                for line in current_part:\n",
    "                    f.write(line + '\\n')\n",
    "            current_part = []\n",
    "            current_size = 0\n",
    "            contador += 1\n",
    "            \n",
    "            if contador > len(tamano_partes):\n",
    "                break\n",
    "\n",
    "    # Guardar la última parte si quedó algo\n",
    "    if current_part:\n",
    "        with open(os.path.join(output_dir, f'parte_{contador}.txt'), 'w', encoding='utf-8') as f:\n",
    "            for line in current_part:\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "# Tamaños deseados para cada parte (en bytes): 100MB, 500MB, 1GB\n",
    "tamano_partes = [100 * 1024 * 1024, 500 * 1024 * 1024, 1 * 1024 * 1024 * 1024]\n",
    "output_dir = 'divided_datasets'\n",
    "\n",
    "# Dividir el dataset\n",
    "dividir_dataset(train_dataset, output_dir, tamano_partes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "# Función de preprocesamiento\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Ruta al directorio con las partes del dataset\n",
    "dataset_parts = [f'divided_datasets/parte_{i}.txt' for i in range(1, len(tamano_partes) + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Entrenar un modelo Word2Vec para cada parte del dataset\n",
    "for i, part in enumerate(dataset_parts):\n",
    "    # Leer y preprocesar el archivo\n",
    "    sentences = LineSentence(part)\n",
    "    \n",
    "    # Entrenar el modelo Word2Vec\n",
    "    model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=10, workers=4, sg=1, epochs=5)\n",
    "    \n",
    "    # Guardar el modelo\n",
    "    model.save(f'word2vec_model_part_{i+1}.model')\n",
    "\n",
    "    print(f'Model for part {i+1} trained and saved.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per entranar el model Word2Vec, ho fem amb diferents mides del Dataset (100MB, 500MB, 1GB, complet). Per fer aquesta divisió, definim la funció dividir_archivo, aquesta funció reb com a paràmetres la ruta de l'arxiu i la mida de les diferents parts (en Bytes), y genera aquests fitxers en la mateixa ruta que l'original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_archivo(ruta_archivo, tamano_partes):\n",
    "    # Abrir el archivo en modo lectura binaria\n",
    "    with open(ruta_archivo, 'rb') as archivo_original:\n",
    "        contador = 1\n",
    "        for tamano_parte in tamano_partes:\n",
    "            with open(ruta_archivo + f\"_parte{contador}.txt\", 'wb') as archivo_parte:\n",
    "                bytes_leidos = 0\n",
    "                while bytes_leidos < tamano_parte:\n",
    "                    # Leer un bloque del tamaño especificado o hasta el final del archivo\n",
    "                    contenido = archivo_original.read(min(4096, tamano_parte - bytes_leidos))\n",
    "                    if not contenido:\n",
    "                        break  # Si no hay más contenido, terminar\n",
    "                    archivo_parte.write(contenido)\n",
    "                    bytes_leidos += len(contenido)\n",
    "            contador += 1\n",
    "\n",
    "# Ruta del archivo a dividir\n",
    "ruta_archivo = 'C:/Users/Usuario/Desktop/uni/Q4/PLH/pract4/catalan_general_crawling/corpus/catalan_general_crawling.txt'\n",
    "# Tamaños deseados para cada parte (en bytes)\n",
    "tamano_partes = [100 * 1024 * 1024, 500 * 1024 * 1024, 1 * 1024 * 1024 * 1024]  # 100 MB, 500 MB, 1 GB\n",
    "\n",
    "# Llamar a la función para dividir el archivo\n",
    "dividir_archivo(ruta_archivo, tamano_partes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vegada tenim les diferents parts creades, passem a preprocessar el text per tal de "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m ruta_parte \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Usuario/Desktop/uni/Q4/PLH/pract4/catalan_general_crawling/corpus/catalan_general_crawling.txt_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparte\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     45\u001b[0m ruta_salida \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Usuario/Desktop/uni/Q4/PLH/pract4/catalan_general_crawling/corpus/catalan_general_crawling_preprocessed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparte\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 46\u001b[0m \u001b[43mpreprocesar_archivo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruta_parte\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mruta_salida\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 38\u001b[0m, in \u001b[0;36mpreprocesar_archivo\u001b[1;34m(ruta_archivo, ruta_salida)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(ruta_salida, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m archivo_salida:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m linea \u001b[38;5;129;01min\u001b[39;00m archivo:\n\u001b[1;32m---> 38\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocesar_texto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinea\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m         archivo_salida\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 28\u001b[0m, in \u001b[0;36mpreprocesar_texto\u001b[1;34m(texto)\u001b[0m\n\u001b[0;32m     25\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string\u001b[38;5;241m.\u001b[39mpunctuation]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Lemmatización con spaCy\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc]\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1049\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\tagger.pyx:138\u001b[0m, in \u001b[0;36mspacy.pipeline.tagger.Tagger.predict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\layers\\with_array.py:42\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](Xseq, is_train)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\layers\\with_array.py:77\u001b[0m, in \u001b[0;36m_list_forward\u001b[1;34m(model, Xs, is_train)\u001b[0m\n\u001b[0;32m     75\u001b[0m lengths \u001b[38;5;241m=\u001b[39m NUMPY_OPS\u001b[38;5;241m.\u001b[39masarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[0;32m     76\u001b[0m Xf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(Xs, pad\u001b[38;5;241m=\u001b[39mpad)\n\u001b[1;32m---> 77\u001b[0m Yf, get_dXf \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYs: ListXd) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ListXd:\n\u001b[0;32m     80\u001b[0m     dYf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(dYs, pad\u001b[38;5;241m=\u001b[39mpad)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\layers\\softmax.py:71\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     69\u001b[0m W \u001b[38;5;241m=\u001b[39m cast(Floats2d, model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     70\u001b[0m b \u001b[38;5;241m=\u001b[39m cast(Floats1d, model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 71\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[0;32m     74\u001b[0m     Y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39msoftmax(Y, temperature\u001b[38;5;241m=\u001b[39mtemperature)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\backends\\ops.py:263\u001b[0m, in \u001b[0;36mOps.affine\u001b[1;34m(self, X, W, b)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maffine\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: Floats2d, W: Floats2d, b: Floats1d) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Floats2d:\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply a weights layer and a bias to some inputs, i.e.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m    Y = X @ W.T + b\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m     Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m b\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Y\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Cargar el modelo de spaCy para catalán\n",
    "nlp = spacy.load('ca_core_news_sm')\n",
    "\n",
    "# Función para preprocesar el texto\n",
    "def preprocesar_texto(texto):\n",
    "    # Convertir a minúsculas\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    # Tokenizar el texto\n",
    "    tokens = word_tokenize(texto)\n",
    "    \n",
    "    # Eliminar puntuación y stopwords\n",
    "    stop_words = set(stopwords.words('catalan'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    \n",
    "    # Lemmatización con spaCy\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Función para preprocesar el archivo\n",
    "def preprocesar_archivo(ruta_archivo, ruta_salida):\n",
    "    with open(ruta_archivo, 'r', encoding='utf-8') as archivo:\n",
    "        with open(ruta_salida, 'w', encoding='utf-8') as archivo_salida:\n",
    "            for linea in archivo:\n",
    "                tokens = preprocesar_texto(linea)\n",
    "                archivo_salida.write(' '.join(tokens) + '\\n')\n",
    "\n",
    "# Preprocesar cada parte del archivo dividido\n",
    "partes = [\"parte1\", \"parte2\", \"parte3\"]\n",
    "for parte in partes:\n",
    "    ruta_parte = f'C:/Users/Usuario/Desktop/uni/Q4/PLH/pract4/catalan_general_crawling/corpus/catalan_general_crawling.txt_{parte}.txt'\n",
    "    ruta_salida = f'C:/Users/Usuario/Desktop/uni/Q4/PLH/pract4/catalan_general_crawling/corpus/catalan_general_crawling_preprocessed_{parte}.txt'\n",
    "    preprocesar_archivo(ruta_parte, ruta_salida)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "# Entrenar el modelo Word2Vec con cada parte preprocesada\n",
    "for parte in partes:\n",
    "    corpus_file = f'C:/Users/Usuario/Desktop/uni/Q4/PLH/pract4/catalan_general_crawling/corpus/catalan_general_crawling_preprocessed_{parte}.txt'\n",
    "    sentences = LineSentence(corpus_file)\n",
    "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=10, workers=4, epochs=25)\n",
    "    model.save(f'word2vec_model_{parte}.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "per cada frase un unic vector\n",
    "TF-IDF per descartar paraules uq no aporten info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def build_and_compile_model(hidden_size: int = 64) -> tf.keras.Model:\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Concatenate(axis=-1, ),\n",
    "      tf.keras.layers.Dense(hidden_size, activation='relu'),\n",
    "      tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "  model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "  return model\n",
    "m = build_and_compile_model()\n",
    "# E.g.\n",
    "import numpy as np\n",
    "y = m((np.ones((1, 100)), np.ones((1,100)), ), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el primer 10 s'ha de canviar per la long maxima del vector d'entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def build_and_compile_model(\n",
    "        input_length: int = 10, hidden_size: int = 64, dictionary_size: int = 1000, embedding_size: int = 16,\n",
    ") -> tf.keras.Model:\n",
    "    input_1, input_2 = tf.keras.Input((input_length, ), dtype=tf.int32, ), tf.keras.Input((input_length, ), dtype=tf.int32, )\n",
    "    # Define Layers\n",
    "    embedding = tf.keras.layers.Embedding(\n",
    "        dictionary_size, embedding_size, input_length=input_length, mask_zero=True, )\n",
    "    pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "    concatenate = tf.keras.layers.Concatenate(axis=-1, )\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "    output = tf.keras.layers.Dense(1)\n",
    "    # Pass through the layers\n",
    "    _input_mask_1, _input_mask_2 = tf.not_equal(input_1, 0), tf.not_equal(input_2, 0)\n",
    "    _embedded_1, _embedded_2 = embedding(input_1, ), embedding(input_2, )\n",
    "    _pooled_1, _pooled_2 = pooling(_embedded_1, mask=_input_mask_1), pooling(_embedded_2, mask=_input_mask_2)\n",
    "    _concatenated = concatenate((_pooled_1, _pooled_2, ))\n",
    "    _hidden_output = hidden(_concatenated)\n",
    "    _output = output(_hidden_output)\n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=(input_1, input_2, ), outputs=_output, )\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
