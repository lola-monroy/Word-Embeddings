{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PrÃ ctica 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenament de models de Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "     ---------------------------------------- 0.0/542.0 kB ? eta -:--:--\n",
      "     ------------- ------------------------ 194.6/542.0 kB 3.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- -- 501.8/542.0 kB 6.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 542.0/542.0 kB 5.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.12.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.25.2)\n",
      "Collecting pyarrow>=12.0.0\n",
      "  Downloading pyarrow-16.1.0-cp311-cp311-win_amd64.whl (25.9 MB)\n",
      "     ---------------------------------------- 0.0/25.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.2/25.9 MB 5.0 MB/s eta 0:00:06\n",
      "      --------------------------------------- 0.6/25.9 MB 6.2 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 0.9/25.9 MB 6.1 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 1.2/25.9 MB 6.2 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 1.4/25.9 MB 6.0 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 1.7/25.9 MB 6.0 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 2.0/25.9 MB 6.0 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 2.2/25.9 MB 6.1 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 2.5/25.9 MB 6.0 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 2.7/25.9 MB 5.9 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 2.9/25.9 MB 5.8 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 3.2/25.9 MB 5.7 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 3.3/25.9 MB 5.6 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 3.6/25.9 MB 5.6 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 3.8/25.9 MB 5.6 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 4.0/25.9 MB 5.4 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 4.3/25.9 MB 5.5 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 4.6/25.9 MB 5.5 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 4.9/25.9 MB 5.6 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 5.2/25.9 MB 5.6 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 5.4/25.9 MB 5.6 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 5.7/25.9 MB 5.7 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 6.0/25.9 MB 5.6 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 6.2/25.9 MB 5.6 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 6.5/25.9 MB 5.6 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 6.6/25.9 MB 5.5 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 6.8/25.9 MB 5.5 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 7.0/25.9 MB 5.5 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 7.2/25.9 MB 5.4 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 7.3/25.9 MB 5.3 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 7.6/25.9 MB 5.4 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 7.9/25.9 MB 5.3 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 8.2/25.9 MB 5.4 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 8.4/25.9 MB 5.4 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 8.7/25.9 MB 5.4 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 9.0/25.9 MB 5.4 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 9.3/25.9 MB 5.4 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 9.6/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 9.9/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 10.1/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 10.4/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 10.7/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 11.0/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 11.2/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 11.5/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 11.7/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 12.0/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 12.4/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 12.7/25.9 MB 5.5 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 13.0/25.9 MB 5.6 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 13.3/25.9 MB 5.6 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 13.6/25.9 MB 5.7 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 13.9/25.9 MB 5.7 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 14.2/25.9 MB 5.8 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 14.6/25.9 MB 5.8 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 14.9/25.9 MB 5.8 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 15.1/25.9 MB 5.8 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 15.5/25.9 MB 5.9 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 15.8/25.9 MB 5.9 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 16.1/25.9 MB 6.0 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 16.3/25.9 MB 5.9 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 16.7/25.9 MB 6.0 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 17.0/25.9 MB 6.1 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 17.3/25.9 MB 6.3 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 17.6/25.9 MB 6.3 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 17.8/25.9 MB 6.3 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 18.1/25.9 MB 6.4 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 18.5/25.9 MB 6.4 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 18.8/25.9 MB 6.4 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 19.1/25.9 MB 6.4 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 19.4/25.9 MB 6.4 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 19.7/25.9 MB 6.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 19.9/25.9 MB 6.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 20.1/25.9 MB 6.3 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 20.3/25.9 MB 6.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 20.6/25.9 MB 6.3 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 20.8/25.9 MB 6.2 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 21.0/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 21.1/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 21.4/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 21.6/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 21.9/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 22.1/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 22.5/25.9 MB 6.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 22.8/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 23.0/25.9 MB 6.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 23.3/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 23.6/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 23.9/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 24.3/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 24.5/25.9 MB 6.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 24.8/25.9 MB 6.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 25.0/25.9 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  25.4/25.9 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  25.7/25.9 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  25.9/25.9 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 25.9/25.9 MB 5.7 MB/s eta 0:00:00\n",
      "Collecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "     ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 116.3/116.3 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.66.2)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl (29 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "     ---------------------------------------- 0.0/143.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 143.5/143.5 kB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2024.3.1)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.9.5-cp311-cp311-win_amd64.whl (370 kB)\n",
      "     ---------------------------------------- 0.0/370.8 kB ? eta -:--:--\n",
      "     ----------------------- -------------- 225.3/370.8 kB 6.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 370.8/370.8 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "     ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.5/50.5 kB ? eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.5-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl (76 kB)\n",
      "     ---------------------------------------- 0.0/76.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 76.7/76.7 kB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, multidict, frozenlist, dill, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 datasets-2.19.1 dill-0.3.8 frozenlist-1.4.1 multidict-6.0.5 multiprocess-0.70.16 pyarrow-16.1.0 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for projecte-aina/catalan_general_crawling contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/projecte-aina/catalan_general_crawling\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c90be95886412686151f7a6bfcd35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885e8832edfc49caab4822b72ad2a32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a40a89e8e947db88aa3466dec5dc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/875M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d26cdbbcde42e787edd7cea04a62be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"projecte-aina/catalan_general_crawling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1016113\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# FunciÃ³n de preprocesamiento\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_y_preprocesar_dataset(dataset, output_dir, tamano_partes):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    total_bytes = 0\n",
    "    contador = 1\n",
    "    current_size = 0\n",
    "    current_part = []\n",
    "    \n",
    "    for i, row in enumerate(dataset):\n",
    "        text = row['text']\n",
    "        preprocessed_text = preprocess(text)\n",
    "        current_size += len(preprocessed_text.encode('utf-8'))\n",
    "        current_part.append(preprocessed_text)\n",
    "        \n",
    "        if current_size >= tamano_partes[contador - 1]:\n",
    "            with open(os.path.join(output_dir, f'parte_{contador}.txt'), 'w', encoding='utf-8') as f:\n",
    "                for line in current_part:\n",
    "                    f.write(line + '\\n')\n",
    "            current_part = []\n",
    "            current_size = 0\n",
    "            contador += 1\n",
    "            \n",
    "            if contador > len(tamano_partes):\n",
    "                break\n",
    "\n",
    "    # Guardar la Ãºltima parte si quedÃ³ algo\n",
    "    if current_part:\n",
    "        with open(os.path.join(output_dir, f'parte_{contador}.txt'), 'w', encoding='utf-8') as f:\n",
    "            for line in current_part:\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "# TamaÃ±os deseados para cada parte (en bytes): 100MB, 500MB, 1GB\n",
    "tamano_partes = [100 * 1024 * 1024, 500 * 1024 * 1024, 1 * 1024 * 1024 * 1024]\n",
    "output_dir = 'divided_datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir y preprocesar el dataset\n",
    "dividir_y_preprocesar_dataset(train_dataset, output_dir, tamano_partes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "# Ruta al directorio con las partes del dataset\n",
    "dataset_parts = [f'divided_datasets/parte_{i}.txt' for i in range(1, len(tamano_partes) + 1)]\n",
    "\n",
    "# Entrenar un modelo Word2Vec para cada parte del dataset\n",
    "for i, part in enumerate(dataset_parts):\n",
    "    # Leer el archivo preprocesado\n",
    "    sentences = LineSentence(part)\n",
    "    \n",
    "    # Entrenar el modelo Word2Vec\n",
    "    model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=10, workers=4, sg=1, epochs=25)\n",
    "    \n",
    "    # Guardar el modelo\n",
    "    model.save(f'word2vec_model_part_{i+1}.model')\n",
    "\n",
    "    print(f'Model for part {i+1} trained and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1016113\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['ReduÃ¯u els costos dels processos administratius al vostre organisme pÃºblic\\nEviteu els desplaÃ§aments i pÃ¨rdua de temps als ciutadans en les seves gestions\\nOferiu una administraciÃ³ mÃ©s transparent a ciutadans i empreses\\nEns grans i petits experimenten aquesta transformaciÃ³ amb Ã¨xit, grÃ cies al suport de l\\'AOC\\nDepartament de Sistemes d\\'InformaciÃ³ i Processos\\n\" Via Oberta ens ha permÃ¨s fer efectiu el dret dels ciutadans a no aportar documents, eliminant paper i simplificant procediments\"\\n\" e.FACT proporciona informaciÃ³ indispensable per a la realitzaciÃ³ de les auditories del registre comptable de factures de les Administracions PÃºbliques Catalanes\"\\nCoordinador del departament d\\'InformÃ tica\\n\"El servei VIA OBERTA Ã©s el que ha aportat majors avantatges per als ciutadans\"\\n\"Amb l\\' e-NOTUM hem escurÃ§at els procediments en 12 dies, quasi un 40% menys!\"\\nCoordinadora d\\'organitzaciÃ³ de persones i e-administraciÃ³\\n\" Via Oberta ofereix millores per als ciutadans al no haver d\\'aportar cap document\"\\nResponsable d\\'InformÃ tica i AdministraciÃ³ ElectrÃ²nica\\n\" e-TRAM ens ha permÃ¨s implantar un servei de tramitaciÃ³ electrÃ²nica per als ciutadans de forma rÃ pida, senzilla i amb un cost reduÃ¯t\"\\n\"Els municipis amb pocs habitants trobem en els serveis de l\\'AOC la gratuÃ¯tat i la comoditat necessÃ ries per dur a terme el nostre dia a dia\"\\n\"Les T-CAT han permÃ¨s incorporar de forma segura la signatura electrÃ²nica dins dels nostres procediments afavorint la transformaciÃ³ digital de la nostra activitat\"\\nCap de Departament de Sistemes i Tecnologies de la InformaciÃ³\\n\"Amb el desplegament de l\\' idCAT hem apropat l\\'Ajuntament a la ciutadania\"\\n\"MitjanÃ§ant els serveis de Govern Obert de l\\'AOC hem pogut fer fÃ cil el que sembla difÃ­cil\"\\n\"Al tauler electrÃ²nic pots penjar fins i tot el projecte sencer i al final et permet fer tambÃ© la diligÃ¨ncia\"\\nÃrea de PromociÃ³ EconÃ²mica, AdministraciÃ³ i Hisenda\\n\"El Sobre Digital i la PSCP han aconseguit una comuniÃ³ senzilla entre empreses i administraciÃ³ per universalitzar la compra pÃºblica electrÃ²nica\"\\n\"L\\' e-SET Ã©s la implantaciÃ³ d\\'un nou sistema de treball que facilita la feina del dia a dia\"\\nCap del servei de contractaciÃ³ i compres\\n\"El Sobre Digital, una experiÃ¨ncia imprescindible per a la bona administraciÃ³ amb estalvi de recursos i millora de la seguretat jurÃ­dica i la transparÃ¨ncia\"\\nÃrea d\\'OrganitzaciÃ³ i AdministraciÃ³ ElectrÃ²nica\\n\"El desplegament de la valisa electrÃ²nica ha estat clau en el procÃ©s de transformaciÃ³ digital dels nostres procediments interns\"\\n\"L\\' HÃ¨stia permet el treball en temps real i des de qualsevol lloc, aixÃ­ com sistematitzar la prÃ ctica professional, recollir la informaciÃ³ ordenadament i amb el mateix llenguatge\"\\nConsulta els materials del CongrÃ©s de Govern Digital 2019\\nGoverns transparents, fluids, dinÃ mics, lÃ­quids... un bon lema pel principal objectiu de la governanÃ§a del segle XXI: democratitzar-ho tot.\\nConfluÃ¨ncies, rius, cooperaciÃ³.\\nCatalunya, MediterrÃ nia, mar de drets.\\nA favor: totes les Administracions movent-se per posar-se al dia i millorar, tot aprofitant la revoluciÃ³ digital.\\nEn contra: quants cops estem reinventant la roda i quantes quantes oportunitats perdudes de fer-ho una Ãºnica vegada i de forma coordinada i colÂ·laborativa?\\n\"La transparÃ¨ncia Ã©s una oportunitat.\\nHem de perdre tota por a explicar quÃ¨ fem\": la conclusiÃ³ de la taula d\\'alcaldies de la Jornada de Govern Obert pic.twitter.com/ERbgLSIXZM\\nEl director general de ParticipaciÃ³ Ciutadana ens convida a transformar les administracions pÃºbliques a partir de la participaciÃ³ ciutadana\\nEns cal que allÃ² que preocupa i ocupa els governants formi part d\\'allÃ² en quÃ¨ participa la ciutadania pic.twitter.com/NwQr4EZSCS: \"A moltes institucions encara els sona xinÃ©s aixÃ² de les dades obertes i la transparÃ¨ncia.\\nDe que serveix que hi hagi un portal, si llavors no hi ha dades?\\nLlavors l\\'accÃ©s a la informaciÃ³ pels periodistes Ã©s molt parcial\".\\nOferim eines que, conjuntament amb la metodologia i el suport necessari, fan possible l\\'assoliment d\\'un govern digital\\nPosem al vostre abast tot el coneixement: formaciÃ³, guies, normatives, etc.\\nTenim eines per gestionar Ã gilment part del procÃ©s administratiu del vostre ens\\nEl nostre equip farÃ  tot el possible per resoldre les vostres incidÃ¨ncies\\nSabem que es tracta d\\'una decisiÃ³ molt important per al vostre ens i Ã©s per aixÃ² que us ho volem posar fÃ cil.\\nLa selecciÃ³ de l\\'actualitat d\\'AdministraciÃ³ Oberta a la vostra safata.',\n",
       "  'En compliment de la Directiva 2009/136/CE, desenvolupada en el nostre ordenament per l\\'article 22.2 de la Llei de Serveis de Societat de la InformaciÃ³ (LSSI) i seguint les instruccions de l\\'AgÃ¨ncia Espanyola de ProtecciÃ³ de Dades, procedim a informar-li detalladament de l\\'Ãºs que es realitza a la nostra pÃ gina web.\\nAquesta informaciÃ³ no revela la seva identitat, perÃ² sÃ­ que permet la seva identificaciÃ³ com a un usuari concret i pot guardar informaciÃ³ relativa a la freqÃ¼Ã¨ncia amb la que visita la pÃ gina web, les seves preferÃ¨ncies de navegaciÃ³ o aquella informaciÃ³ que mÃ©s l\\'interessa.\\nEl que ens permet, cada vegada que accedeix a www.aoc.cat, millorar la qualitat i la usabilitat de la nostra pÃ gina web.\\nNo obstant, si les desactiva, pot ser que la seva navegaciÃ³ per www.aoc.cat no sigui Ã²ptima i algunes de les seves utilitats no funcionin correctament.\\nCookies analÃ­tiques: galetes de Google Analytics\\nAquesta pÃ gina web utilitza Google Analytics, un servei analÃ­tic del web prestat per Google.\\nInc, una companyia de Delaware l\\'oficina principal de la qual es troba a 1600 Amphitheatre Parkway, Mountain View (CalifÃ²rnia), CA 94043, Estats Units (\" Google\").\\nLa informaciÃ³ que genera la cookie sobre l\\'Ãºs del lloc web (incloent l\\'adreÃ§a IP) serÃ  directament transmesa i arxivada per Google en els seus servidors d\\'Estats Units.\\nGoogle utilitzarÃ  aquesta informaciÃ³ per compte nostre amb el propÃ²sit de seguir la pista del seu Ãºs del lloc web.\\nGoogle podrÃ  transmetre aquesta informaciÃ³ a tercers quan aixÃ­ ho requereixi la legislaciÃ³, o quan aquests tercers processin la informaciÃ³ per compte de Google.\\nEn aquests casos, Google no associarÃ  la seva adreÃ§a IP amb cap altra dada de quÃ¨ disposi.\\nEn utilitzar aquesta pÃ gina web consent el tractament de la seva informaciÃ³ per Google en la forma i per als fins anteriorment indicats.\\nL\\'exercici de qualsevol dret s\\'haurÃ  de realitzar mitjanÃ§ant comunicaciÃ³ directa amb Google.\\nPer optar per no ser rastrejats per Google Analytics a travÃ©s de tots els llocs web podeu consultar http://tools.google.com/dlpage/gaoptout\\nAixÃ­ mateix, tambÃ© registra quan va ser la primera i l\\'Ãºltima vegada que l\\'usuari va visitar el web www.aoc.cat.\\nGaletes en altres llocs web del Consorci AOC\\nLes seves finalitats sÃ³n descrites a la pÃ gina de privacitat de Twitter.\\nConfiguraciÃ³ de l\\'usuari per evitar Cookies\\nÃs cas de dubte pot dirigir-se al webmaster del domini creador de la cookie.\\nLa selecciÃ³ de l\\'actualitat d\\'AdministraciÃ³ Oberta a la vostra safata.',\n",
       "  'L\\'Ãºs de la informaciÃ³ continguda en aquest lloc web implica l\\'acceptaciÃ³ i el consentiment en els termes i les condicions que es detallen en aquest avÃ­s legal.\\nTitularitat i rÃ¨gim de responsabilitat de la pÃ gina web\\nEl responsable d\\'aquesta pÃ gina web Ã©s el Consorci AdministraciÃ³ Oberta de Catalunya, (Consorci AOC), amb NIF Q0801175A, i ubicat al Carrer de TÃ nger, nÃºm. 98, (planta baixa) 08018 (tel. 93 272 25 00 i fax.\\nTota persona que accedeixi a aquest lloc web assumeix el paper d\\'usuari, comprometent-se a l\\'observanÃ§a i compliment rigorÃ³s de les disposicions aquÃ­ disposades, aixÃ­ com a qualsevol altre disposiciÃ³ legal que li sigui d\\'aplicaciÃ³.\\nEl Consorci AOC tÃ© el dret a modificar la informaciÃ³ que apareix en aquesta pÃ gina web, sense que existeixi la obligaciÃ³ de preavÃ­s o posada en coneixement dels usuaris de les noves obligacions -a excepciÃ³ dels compromisos assumits en virtut de convenis especÃ­fics â entenent-se com a suficient la seva publicaciÃ³ en el lloc web.\\nLes informacions i els continguts relacionats amb l\\'actuaciÃ³ i les funcions del Consorci AOC, que s\\'inclouen en aquest web, estan subjectes a les previsions segÃ¼ents:\\nResponsabilitat amb relaciÃ³ als continguts\\nEl Consorci AOC treballa perquÃ¨ les informacions, els continguts i els serveis oferts o difosos en aquest web acompleixin de manera suficient la necessÃ ria integritat, veracitat, actualitzaciÃ³, accessibilitat i usabilitat.\\nA aquest efecte, cal tenir en compte la data d\\'actualitzaciÃ³ de cadascun dels continguts que en cada cas s\\'indiqui.\\nLa pÃ gina web ofereix informaciÃ³, consells, guies i d\\'altres continguts preparats pel Consorci AOC amb finalitats de difusiÃ³, informaciÃ³, conscienciaciÃ³ i en determinats casos la prestaciÃ³ de serveis especÃ­fics d\\'administraciÃ³ electrÃ²nica.\\nS\\'informa a l\\'usuari que tots aquests continguts, malgrat estar preparats amb el mÃ xim nivell de qualitat possibles, no poden suposar en cap moment assessorament especÃ­fic en matÃ¨ria tecnolÃ²gica i/o jurÃ­dica o ser considerats com a actuacions dirigides a la resoluciÃ³ de problemÃ tiques especÃ­fiques.\\nEn qualsevol cas, el Consorci AOC es reserva el dret a modificar-los, suprimir-los, desenvolupar-los o actualitzar-los unilateralment sense notificaciÃ³ prÃ¨via i sense assumir cap responsabilitat.\\nEl Consorci AOC ofereix la traducciÃ³ automÃ tica en altres llengÃ¼es diferents del catalÃ  dels continguts del seu web per tal de facilitar als ciutadans la comprensiÃ³ del text en el seu propi idioma.\\nMalgrat tot, els articles traduÃ¯ts automÃ ticament poden contenir errors materials dels quals el Consorci AOC no se\\'n fa responsable.\\nEn l\\'actualitat, el Consorci AOC nomÃ©s garanteix la veracitat dels continguts en llengua catalana.\\nReferÃ¨ncies i enllaÃ§os a webs d\\'altres organitzacions\\nEl web del Consorci AOC contÃ© referÃ¨ncies o enllaÃ§os a webs de tercers (\"links\"), la major part d\\'elles sÃ³n a pÃ gines d\\'Internet d\\'altres administracions pÃºbliques, que s\\'han considerat d\\'interÃ¨s pels usuaris.\\nEn el cas que s\\'abandoni el web, el Consorci AOC no assumeix cap responsabilitat derivada de la connexiÃ³ o dels contingut dels enllaÃ§os de tercers.\\nTot i aixÃ², es revisen periÃ²dicament els enllaÃ§os a altres pÃ gines per tal d\\'evitar la inclusiÃ³ d\\'enllaÃ§os que no compleixen la normativa de protecciÃ³ de dades, aixÃ­ com la resta de normativa vigent.\\nEn aquest sentit, el Consorci manifesta que si es detecta qualsevol contingut que pugui contravenir la legislaciÃ³ nacional o internacional, o l\\'ordre pÃºblic, es procedirÃ  a la retirada immediata de l\\'enllaÃ§, posant-ho en coneixement de les autoritat competents.\\nEl Consorci AOC no es fa responsable de la informaciÃ³ i continguts emmagatzemats, a tÃ­tol enunciatiu perÃ² no limitatiu, en els fÃ²rums, blocs, comentaris en xarxes socials, o qualsevol altre mitjÃ  del Consorci AOC que permeti a tercers publicar continguts de forma independent.\\nNo obstant, en compliment de l\\'article 11 i 16 de la LSSICE, es posa a disposiciÃ³ de tots els usuaris, autoritats i forces de seguretat, per colÂ·laborar de forma activa en la retirada o, en el seu cas, bloqueig de tots aquells continguts que poguessin afectar o contravenir la legislaciÃ³ nacional, o internacional, drets de tercers o la moral i l\\'ordre pÃºblic.\\nEn el cas que qualsevol usuari consideri que existeix en el lloc web algun contingut que pugui ser susceptible de l\\'anterior classificaciÃ³, es solÂ·licita que ho comuniqui de forma immediata a l\\'administrador del la pÃ gina web per mitjÃ  del correu electrÃ²nic [EMAIL]\\nReproducciÃ³ de continguts propis\\nEl Consorci AOC facilita la consulta lliure i gratuÃ¯ta de la informaciÃ³ continguda en el web i autoritza la reproducciÃ³ total o parcial dels seus continguts, sempre i quan els continguts esmentats es conservin Ã­ntegres, es citi la font i la data en la que s\\'ha realitzat la cÃ²pia, no es manipulin, ni alterin els continguts i no s\\'utilitzi directament amb finalitats comercials (Llei 37/2007, de 16 de novembre, sobre la reutilitzaciÃ³ de la informaciÃ³ del sector pÃºblic).\\nEl Consorci AOC autoritza la descarrega gratuÃ¯ta dels manuals, impresos, programes i publicacions informatives que s\\'inclouen en el seu web, a efectes de la seva reproducciÃ³ i distribuciÃ³, llevat que s\\'indiqui el contrari de forma expressa.\\nNo obstant aixÃ², en determinats supÃ²sits el Consorci AOC pot indicar de manera explÃ­cita que Ã©s necessari solÂ·licitar una autoritzaciÃ³ expressa.\\nAixÃ­ mateix, la reutilitzaciÃ³ es pot limitar per la tutela d\\'altres bÃ©ns jurÃ­dics prioritaris, com ara la protecciÃ³ de les dades personals, la intimitat o els drets de protecciÃ³ intelÂ·lectual de tercers.\\nEl domini d\\'aquest web Ã©s titularitat del Consorci AOC, aixÃ­ com els drets de propietat intelÂ·lectual, el seu disseny i els codis que contÃ©, llevat que s\\'indiqui una titularitat diferent.\\nNo s\\'autoritza en cap cas, l\\'Ãºs de marques o signes distintius, logotips i en general sÃ­mbols distintius de qualsevol naturalesa propietat del Consorci AOC, en publicacions i webs que no siguin d\\'ens participats o patrocinats per aquest Consorci, sense el coneixement i l\\'autoritzaciÃ³ corresponent del Consorci AOC.\\nEl Consorci AOC no assumirÃ  cap responsabilitat derivada de l\\'Ãºs per part de tercers del contingut d\\'aquesta pÃ gina Web i podrÃ  exercitar totes les accions civils o penals que li corresponguin en cas d\\'infracciÃ³ d\\'aquests drets per part de l\\'usuari.\\nAquesta informaciÃ³ es troba continguda a la PolÃ­tica de Cookies del Consorci AOC.\\nDret aplicable i jurisdicciÃ³ competent\\nLa llei aplicable en cas de disputa o conflicte d\\'interpretaciÃ³ dels termes que conforme aquest AvÃ­s legal, aixÃ­ com qualsevol aspecte relacionat amb els serveis d\\'aquest web, serÃ  la legislaciÃ³ espanyola.\\nEls possibles conflictes relatius a aquest web es regiran exclusivament pel dret espanyol, essent els jutjats de Barcelona els Ãºnics competents.\\nTota persona usuÃ ria del web, independentment de la jurisdicciÃ³ territorial des de la qual es produeixi el seu accÃ©s, accepta el compliment i respecte d\\'aquesta clÃ usula amb renÃºncia expressa a qualsevol altre fur que li poguÃ©s correspondre.\\nSi alguna part o clÃ usula d\\'aquestes Condicions fos declarada nulÂ·la o deixada sense efecte per una resoluciÃ³ judicial, les restants estipulacions conservaran la seva validesa.\\nLa selecciÃ³ de l\\'actualitat d\\'AdministraciÃ³ Oberta a la vostra safata.',\n",
       "  \"Els Reconeixements AdministraciÃ³ Oberta als ajuntaments i consells comarcals que atorga anualment l'AOC han esdevingut el reconeixement pÃºblic de tots aquells ens que destaquen en la transformaciÃ³ digital de la seva relaciÃ³ amb la ciutadania i en la seva gestiÃ³ interna.\\nEls guardons s'atorguen en base a uns indicadors objectius, l'anÃ lisi dels webs dels ens i l'Ãºs de determinats serveis del Consorci AOC.\\nEl seu objectiu Ã©s valorar i reconÃ¨ixer la implantaciÃ³ i l'Ãºs dels serveis d'administraciÃ³ electrÃ²nica i, tal i com s'ha indicat anteriorment, la seva conseqÃ¼ent transformaciÃ³ digital en seva relaciÃ³ amb la ciutadania i en la seva gestiÃ³ interna.\\nEls Reconeixements AdministraciÃ³ Oberta tenen set categories d'acord amb el nombre d'habitants dels ens i la seva naturalesa:\\nDescripciÃ³ del mÃ¨tode d'avaluaciÃ³ dels Reconeixements AdministraciÃ³ Oberta\\nLlistat de guardonats a l'ediciÃ³ del 2018: ajuntaments i consells comarcals capdavanters en administraciÃ³ digital\\nSegells Reconeixements AdministraciÃ³ Oberta 2018\\nLa selecciÃ³ de l'actualitat d'AdministraciÃ³ Oberta a la vostra safata.\",\n",
       "  \"En el marc del desenvolupament de l'AdministraciÃ³ electrÃ²nica, des de l'AOC copsem l'estat de l'administraciÃ³ pel que fa als instruments municipals d'AdministraciÃ³ electrÃ²nica i, mÃ©s especÃ­ficament, al reconeixement efectiu dels drets dels ciutadans arrel de la normativa que regula l'Ãºs dels mitjans electrÃ²nics, la transparÃ¨ncia, l'accÃ©s a la informaciÃ³ pÃºblica i el bon govern.\\nAmb aquest objectiu, periÃ²dicament duem a terme una revisiÃ³ de l'estat de l'e-AdministraciÃ³ als 947 ajuntaments de Catalunya 1 i publiquem les dades en un informe.\\nAixÃ­ mateix, els resultats de l'anÃ lisi es plasmen en un mapa de Catalunya interactiu.\\nAl mapa municipal catalÃ  podeu accedir a la informaciÃ³ relacionada amb els 947 municipis catalans:\\nLa informaciÃ³ que mostren els mapes Ã©s la mÃ©s recent.\\nSi voleu conÃ¨ixer les dades dels estudis anteriors podeu accedir als informes sobre administraciÃ³.\\nTambÃ© podeu consultar les dades dels 42 consells comarcals al mapa d'e-administraciÃ³ per comarques.\\nEls informes sobre e-AdministraciÃ³ sÃ³n el resultat d'un estudi dut a terme pel personal del Gabinet TÃ¨cnic de l'AOC en un perÃ­ode de temps concret i en base a:\\nA continuaciÃ³ detallem els parÃ metres analitzats a l'estudi, agrupats en sis blocs\\nSi l'ajuntament ha aprovat normativa en matÃ¨ria d'administraciÃ³ electrÃ²nica\\nSi l'ajuntament ha aprovat alguna norma especÃ­fica per regular l'Ãºs dels mitjans electrÃ²nics en la seva administraciÃ³ (ordenanÃ§a reguladora d'administraciÃ³ electrÃ²nica, registre electrÃ²nic i seu electrÃ²nica).\\nEl resultat d'aquesta anÃ lisi es contrasta amb el personal tÃ¨cnic dels consells comarcals, d'acord amb el conveni de colÂ·laboraciÃ³ que hi ha entre els consells i el Consorci AOC.\\n1 En els tres primers informes de 2010, es van recollir dades de 946 ajuntaments, ja que La Canonja encara no era legalment municipi independent.\\nTambÃ© es va analitzar en el seu moment el municipi de MedinyÃ  fins a la seva forÃ§osa desapariciÃ³ el febrer de 2018.\\nEvoluciÃ³ de les dades dels informes sobre l'e-AdministraciÃ³ (desembre 2017) (977 kB)\\nNOTA: les dades recollides en aquests informes es publiquen ara en format de dades obertes a indicadors pÃºblics d'activitat\\nLa selecciÃ³ de l'actualitat d'AdministraciÃ³ Oberta a la vostra safata.\"]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1016113\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def dividir_dataset(dataset, output_dir, tamano_partes):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    total_bytes = 0\n",
    "    contador = 1\n",
    "    current_size = 0\n",
    "    current_part = []\n",
    "    \n",
    "    for i, row in enumerate(dataset):\n",
    "        text = row['text']\n",
    "        current_size += len(text.encode('utf-8'))\n",
    "        current_part.append(text)\n",
    "        \n",
    "        if current_size >= tamano_partes[contador - 1]:\n",
    "            with open(os.path.join(output_dir, f'parte_{contador}.txt'), 'w', encoding='utf-8') as f:\n",
    "                for line in current_part:\n",
    "                    f.write(line + '\\n')\n",
    "            current_part = []\n",
    "            current_size = 0\n",
    "            contador += 1\n",
    "            \n",
    "            if contador > len(tamano_partes):\n",
    "                break\n",
    "\n",
    "    # Guardar la Ãºltima parte si quedÃ³ algo\n",
    "    if current_part:\n",
    "        with open(os.path.join(output_dir, f'parte_{contador}.txt'), 'w', encoding='utf-8') as f:\n",
    "            for line in current_part:\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "# TamaÃ±os deseados para cada parte (en bytes): 100MB, 500MB, 1GB\n",
    "tamano_partes = [100 * 1024 * 1024, 500 * 1024 * 1024, 1 * 1024 * 1024 * 1024]\n",
    "output_dir = 'divided_datasets'\n",
    "\n",
    "# Dividir el dataset\n",
    "dividir_dataset(train_dataset, output_dir, tamano_partes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "# FunciÃ³n de preprocesamiento\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Ruta al directorio con las partes del dataset\n",
    "dataset_parts = [f'divided_datasets/parte_{i}.txt' for i in range(1, len(tamano_partes) + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['divided_datasets/parte_1.txt',\n",
       " 'divided_datasets/parte_2.txt',\n",
       " 'divided_datasets/parte_3.txt']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divided_datasets/parte_1.txt\n",
      "divided_datasets/parte_2.txt\n",
      "divided_datasets/parte_3.txt\n"
     ]
    }
   ],
   "source": [
    "for part in dataset_parts:\n",
    "    print(part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, f'parte_{contador}.txt'), 'w', encoding='utf-8') as f:\n",
    "                for line in current_part:\n",
    "                    f.write(' '.join(line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Entrenar un modelo Word2Vec para cada parte del dataset\n",
    "for i, part in enumerate(dataset_parts):\n",
    "    # Leer y preprocesar el archivo\n",
    "    sentences = LineSentence(part)\n",
    "    \n",
    "    # Entrenar el modelo Word2Vec\n",
    "    model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=10, workers=4, sg=1, epochs=5)\n",
    "    \n",
    "    # Guardar el modelo\n",
    "    model.save(f'word2vec_model_part_{i+1}.model')\n",
    "\n",
    "    print(f'Model for part {i+1} trained and saved.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per entranar el model Word2Vec, ho fem amb diferents mides del Dataset (100MB, 500MB, 1GB, complet). Per fer aquesta divisiÃ³, definim la funciÃ³ dividir_archivo, aquesta funciÃ³ reb com a parÃ metres la ruta de l'arxiu i la mida de les diferents parts (en Bytes), y genera aquests fitxers en la mateixa ruta que l'original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_archivo(ruta_archivo, tamano_partes):\n",
    "    # Abrir el archivo en modo lectura binaria\n",
    "    with open(ruta_archivo, 'rb') as archivo_original:\n",
    "        contador = 1\n",
    "        for tamano_parte in tamano_partes:\n",
    "            with open(ruta_archivo + f\"_parte{contador}.txt\", 'wb') as archivo_parte:\n",
    "                bytes_leidos = 0\n",
    "                while bytes_leidos < tamano_parte:\n",
    "                    # Leer un bloque del tamaÃ±o especificado o hasta el final del archivo\n",
    "                    contenido = archivo_original.read(min(4096, tamano_parte - bytes_leidos))\n",
    "                    if not contenido:\n",
    "                        break  # Si no hay mÃ¡s contenido, terminar\n",
    "                    archivo_parte.write(contenido)\n",
    "                    bytes_leidos += len(contenido)\n",
    "            contador += 1\n",
    "\n",
    "# Ruta del archivo a dividir\n",
    "ruta_archivo = 'C:/Users/Usuario/Desktop/uni/Q4/PLH/pract4/catalan_general_crawling/corpus/catalan_general_crawling.txt'\n",
    "# TamaÃ±os deseados para cada parte (en bytes)\n",
    "tamano_partes = [100 * 1024 * 1024, 500 * 1024 * 1024, 1 * 1024 * 1024 * 1024]  # 100 MB, 500 MB, 1 GB\n",
    "\n",
    "# Llamar a la funciÃ³n para dividir el archivo\n",
    "dividir_archivo(ruta_archivo, tamano_partes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vegada tenim les diferents parts creades, passem a preprocessar el text per tal de "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m ruta_parte \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Usuario/Desktop/uni/Q4/PLH/pract4/catalan_general_crawling/corpus/catalan_general_crawling.txt_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparte\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     45\u001b[0m ruta_salida \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Usuario/Desktop/uni/Q4/PLH/pract4/catalan_general_crawling/corpus/catalan_general_crawling_preprocessed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparte\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 46\u001b[0m \u001b[43mpreprocesar_archivo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruta_parte\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mruta_salida\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 38\u001b[0m, in \u001b[0;36mpreprocesar_archivo\u001b[1;34m(ruta_archivo, ruta_salida)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(ruta_salida, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m archivo_salida:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m linea \u001b[38;5;129;01min\u001b[39;00m archivo:\n\u001b[1;32m---> 38\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocesar_texto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinea\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m         archivo_salida\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 28\u001b[0m, in \u001b[0;36mpreprocesar_texto\u001b[1;34m(texto)\u001b[0m\n\u001b[0;32m     25\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string\u001b[38;5;241m.\u001b[39mpunctuation]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# LemmatizaciÃ³n con spaCy\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc]\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1049\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\tagger.pyx:138\u001b[0m, in \u001b[0;36mspacy.pipeline.tagger.Tagger.predict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\layers\\with_array.py:42\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](Xseq, is_train)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\layers\\with_array.py:77\u001b[0m, in \u001b[0;36m_list_forward\u001b[1;34m(model, Xs, is_train)\u001b[0m\n\u001b[0;32m     75\u001b[0m lengths \u001b[38;5;241m=\u001b[39m NUMPY_OPS\u001b[38;5;241m.\u001b[39masarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[0;32m     76\u001b[0m Xf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(Xs, pad\u001b[38;5;241m=\u001b[39mpad)\n\u001b[1;32m---> 77\u001b[0m Yf, get_dXf \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYs: ListXd) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ListXd:\n\u001b[0;32m     80\u001b[0m     dYf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(dYs, pad\u001b[38;5;241m=\u001b[39mpad)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\layers\\softmax.py:71\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     69\u001b[0m W \u001b[38;5;241m=\u001b[39m cast(Floats2d, model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     70\u001b[0m b \u001b[38;5;241m=\u001b[39m cast(Floats1d, model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 71\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[0;32m     74\u001b[0m     Y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39msoftmax(Y, temperature\u001b[38;5;241m=\u001b[39mtemperature)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\backends\\ops.py:263\u001b[0m, in \u001b[0;36mOps.affine\u001b[1;34m(self, X, W, b)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maffine\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: Floats2d, W: Floats2d, b: Floats1d) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Floats2d:\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply a weights layer and a bias to some inputs, i.e.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m    Y = X @ W.T + b\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m     Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m b\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Y\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Cargar el modelo de spaCy para catalÃ¡n\n",
    "nlp = spacy.load('ca_core_news_sm')\n",
    "\n",
    "# FunciÃ³n para preprocesar el texto\n",
    "def preprocesar_texto(texto):\n",
    "    # Convertir a minÃºsculas\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    # Tokenizar el texto\n",
    "    tokens = word_tokenize(texto)\n",
    "    \n",
    "    # Eliminar puntuaciÃ³n y stopwords\n",
    "    stop_words = set(stopwords.words('catalan'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    \n",
    "    # LemmatizaciÃ³n con spaCy\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# FunciÃ³n para preprocesar el archivo\n",
    "def preprocesar_archivo(ruta_archivo, ruta_salida):\n",
    "    with open(ruta_archivo, 'r', encoding='utf-8') as archivo:\n",
    "        with open(ruta_salida, 'w', encoding='utf-8') as archivo_salida:\n",
    "            for linea in archivo:\n",
    "                tokens = preprocesar_texto(linea)\n",
    "                archivo_salida.write(' '.join(tokens) + '\\n')\n",
    "\n",
    "# Preprocesar cada parte del archivo dividido\n",
    "partes = [\"parte1\", \"parte2\", \"parte3\"]\n",
    "for parte in partes:\n",
    "    ruta_parte = f'C:/Users/Usuario/Desktop/uni/Q4/PLH/pract4/catalan_general_crawling/corpus/catalan_general_crawling.txt_{parte}.txt'\n",
    "    ruta_salida = f'C:/Users/Usuario/Desktop/uni/Q4/PLH/pract4/catalan_general_crawling/corpus/catalan_general_crawling_preprocessed_{parte}.txt'\n",
    "    preprocesar_archivo(ruta_parte, ruta_salida)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "# Entrenar el modelo Word2Vec con cada parte preprocesada\n",
    "for parte in partes:\n",
    "    corpus_file = f'C:/Users/Usuario/Desktop/uni/Q4/PLH/pract4/catalan_general_crawling/corpus/catalan_general_crawling_preprocessed_{parte}.txt'\n",
    "    sentences = LineSentence(corpus_file)\n",
    "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=10, workers=4, epochs=25)\n",
    "    model.save(f'word2vec_model_{parte}.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "per cada frase un unic vector\n",
    "TF-IDF per descartar paraules uq no aporten info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def build_and_compile_model(hidden_size: int = 64) -> tf.keras.Model:\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Concatenate(axis=-1, ),\n",
    "      tf.keras.layers.Dense(hidden_size, activation='relu'),\n",
    "      tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "  model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "  return model\n",
    "m = build_and_compile_model()\n",
    "# E.g.\n",
    "import numpy as np\n",
    "y = m((np.ones((1, 100)), np.ones((1,100)), ), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el primer 10 s'ha de canviar per la long maxima del vector d'entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def build_and_compile_model(\n",
    "        input_length: int = 10, hidden_size: int = 64, dictionary_size: int = 1000, embedding_size: int = 16,\n",
    ") -> tf.keras.Model:\n",
    "    input_1, input_2 = tf.keras.Input((input_length, ), dtype=tf.int32, ), tf.keras.Input((input_length, ), dtype=tf.int32, )\n",
    "    # Define Layers\n",
    "    embedding = tf.keras.layers.Embedding(\n",
    "        dictionary_size, embedding_size, input_length=input_length, mask_zero=True, )\n",
    "    pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "    concatenate = tf.keras.layers.Concatenate(axis=-1, )\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "    output = tf.keras.layers.Dense(1)\n",
    "    # Pass through the layers\n",
    "    _input_mask_1, _input_mask_2 = tf.not_equal(input_1, 0), tf.not_equal(input_2, 0)\n",
    "    _embedded_1, _embedded_2 = embedding(input_1, ), embedding(input_2, )\n",
    "    _pooled_1, _pooled_2 = pooling(_embedded_1, mask=_input_mask_1), pooling(_embedded_2, mask=_input_mask_2)\n",
    "    _concatenated = concatenate((_pooled_1, _pooled_2, ))\n",
    "    _hidden_output = hidden(_concatenated)\n",
    "    _output = output(_hidden_output)\n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=(input_1, input_2, ), outputs=_output, )\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
